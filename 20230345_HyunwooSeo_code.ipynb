{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EE331 Machine Learning - Final Project\n",
        "## Heart Attack Risk Prediction\n",
        "\n",
        "Download dataset here: https://www.kaggle.com/datasets/iamsouravbanerjee/heart-attack-prediction-dataset\n",
        "\n",
        "**Project Overview:**\n",
        "This project involves building machine learning models to predict heart attack risk based on patient health data.\n",
        "\n",
        "**Three Main Tasks:**\n",
        "1. **Task 1**: Achieve the best prediction performance\n",
        "2. **Task 2**: Minimize memory usage while maintaining accuracy ≥ 60%\n",
        "3. **Task 3**: Achieve the best performance without using neural networks\n",
        "\n",
        "**Additional Requirements:**\n",
        "- Error analysis of your models\n",
        "- Data visualization using clustering"
      ],
      "metadata": {
        "id": "5bcqnenS2RIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Library Imports\n",
        "\n",
        "Import all necessary libraries for data processing, visualization, and machine learning.\n",
        "\n",
        "**Key libraries:**\n",
        "- `numpy`, `pandas`: Data manipulation\n",
        "- `matplotlib`, `seaborn`: Visualization\n",
        "- `sklearn`: Preprocessing and evaluation tools (You MUST implement all models yourself, without using `sklearn`)"
      ],
      "metadata": {
        "id": "MJ7SM3uc2V2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5SaRwdLq2M-"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pickle\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Loading from Google Drive\n",
        "\n",
        "**Instructions:**\n",
        "1. Upload the dataset `heart_attack_prediction_dataset.csv` to your Google Drive\n",
        "2. Update the `DATA_PATH` variable with your file location\n",
        "3. Run the cell to mount Google Drive and load the dataset\n",
        "\n",
        "**Expected output:**\n",
        "- Dataset shape and basic information\n",
        "- Confirmation of successful loading"
      ],
      "metadata": {
        "id": "4Dj-33S02X_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: Update this path to your dataset location in Google Drive\n",
        "DATA_PATH = '/content/drive/MyDrive/EE331/heart_attack_dataset_updated.csv'\n",
        "\n",
        "print(\"\\nLoading dataset...\")\n",
        "try:\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"  Dataset loaded successfully!\")\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Samples: {len(df)}\")\n",
        "    print(f\"  Features: {len(df.columns)-2} (excluding Patient ID and Label)\")\n",
        "except FileNotFoundError:\n",
        "    print(\"  Error: Dataset file not found!\")\n",
        "    print(\"  Please update DATA_PATH with your correct file location.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "RekHW2Eux_GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Analysis\n",
        "\n",
        "Understanding your data is crucial before building models.\n",
        "\n",
        "**What to look for:**\n",
        "- Dataset size and structure\n",
        "- Data types of each feature\n",
        "- Missing values\n",
        "- Label distribution (class imbalance)\n",
        "- Statistical summary of numerical features"
      ],
      "metadata": {
        "id": "FE6Ix9Ds2aQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Analysis\n",
        "print(\"=\"*80)\n",
        "print(\"DATA ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Display basic information\n",
        "print(\"\\n1. Dataset Information:\")\n",
        "print(f\"   Total samples: {len(df)}\")\n",
        "print(f\"   Total columns: {len(df.columns)}\")\n",
        "print(f\"\\n   Column names:\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"   {i:2d}. {col}\")\n",
        "\n",
        "# Check data types\n",
        "print(f\"\\n2. Data Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\n3. Missing Values:\")\n",
        "missing = df.isnull().sum()\n",
        "if missing.sum() == 0:\n",
        "    print(\"   No missing values found!\")\n",
        "else:\n",
        "    print(missing[missing > 0])\n",
        "\n",
        "# Label distribution\n",
        "print(f\"\\n4. Label Distribution (Heart Attack Risk):\")\n",
        "label_counts = df['Heart Attack Risk'].value_counts()\n",
        "print(f\"   Class 0 (No Risk): {label_counts[0]} ({label_counts[0]/len(df)*100:.2f}%)\")\n",
        "print(f\"   Class 1 (Risk):    {label_counts[1]} ({label_counts[1]/len(df)*100:.2f}%)\")\n",
        "print(f\"   Class Imbalance Ratio: {label_counts[0]/label_counts[1]:.2f}:1\")\n",
        "\n",
        "# Statistical summary for numerical features\n",
        "print(f\"\\n5. Numerical Features Statistics:\")\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "print(df[numerical_cols].describe())"
      ],
      "metadata": {
        "id": "pZMWiv-cyUsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Data Visualization\n",
        "\n",
        "Visualize the data to gain insights.\n",
        "\n",
        "**Visualizations included:**\n",
        "- Label distribution (bar plot and pie chart)"
      ],
      "metadata": {
        "id": "9lNYCzcm2dC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Visualization\n",
        "print(\"=\"*80)\n",
        "print(\"DATA VISUALIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Label distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar plot\n",
        "label_counts.plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
        "axes[0].set_title('Heart Attack Risk Distribution')\n",
        "axes[0].set_xlabel('Heart Attack Risk')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['No Risk (0)', 'Risk (1)'], rotation=0)\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(label_counts, labels=['No Risk (0)', 'Risk (1)'], autopct='%1.1f%%',\n",
        "            colors=['green', 'red'], startangle=90)\n",
        "axes[1].set_title('Heart Attack Risk Proportion')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "65m4frwdyn-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Data Preprocessing (This is just an example. You may modify this if you want.)\n",
        "\n",
        "Clean and transform the data for machine learning.\n",
        "\n",
        "**Preprocessing steps:**\n",
        "1. Remove Patient ID (not a predictive feature)\n",
        "2. Split Blood Pressure into Systolic and Diastolic components\n",
        "\n",
        "**(We recommend re-encoding it using your own method.)**"
      ],
      "metadata": {
        "id": "bNFhaBuu2gqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "print(\"=\"*80)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Step 1: Remove Patient ID\n",
        "print(\"\\n1. Removing Patient ID...\")\n",
        "df_processed = df_processed.drop('Patient ID', axis=1)\n",
        "print(\"     Patient ID removed\")\n",
        "\n",
        "# Step 2: Split Blood Pressure into Systolic and Diastolic\n",
        "print(\"\\n2. Splitting Blood Pressure...\")\n",
        "df_processed[['Systolic_BP', 'Diastolic_BP']] = df_processed['Blood Pressure'].str.split('/', expand=True).astype(int)\n",
        "df_processed = df_processed.drop('Blood Pressure', axis=1)\n",
        "print(\"     Blood Pressure split into Systolic_BP and Diastolic_BP\")\n",
        "\n",
        "# Step 3: Categorical Data One-hot encoding\n",
        "print(\"\\n3. Categorical Data One hot encoding\")\n",
        "categorical_columns = [\"Sex\", \"Diet\", \"Country\", \"Continent\", \"Hemisphere\"]\n",
        "\n",
        "df_processed = pd.get_dummies(\n",
        "    df_processed,\n",
        "    columns=categorical_columns,\n",
        "    drop_first=True\n",
        ").astype(int)\n",
        "\n",
        "print(\"   -> One-hot encoding completed.\")\n",
        "\n",
        "\n",
        "print(\"\\n4. Processed data summary:\")\n",
        "print(f\"   Shape after preprocessing: {df_processed.shape}\")\n",
        "print(\"\\n   Column names after preprocessing:\")\n",
        "for i, col in enumerate(df_processed.columns, 1):\n",
        "    print(f\"   {i:2d}. {col}\")\n",
        "\n",
        "# 숫자형/비숫자형 타입 체크 (모델 입력이 모두 숫자인지 확인용)\n",
        "non_numeric = df_processed.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "if len(non_numeric) == 0:\n",
        "    print(\"\\n   All remaining features are numeric (good for most ML models).\")\n",
        "else:\n",
        "    print(\"\\n   Warning: Non-numeric columns still present:\")\n",
        "    print(\"   \", non_numeric)\n"
      ],
      "metadata": {
        "id": "kd5ke7cWyzIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 숫자형 컬럼 전체\n",
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 2) 이진(0/1) 컬럼 찾기\n",
        "binary_cols = [col for col in numeric_cols\n",
        "               if df_processed[col].dropna().nunique() == 2\n",
        "               and set(df_processed[col].dropna().unique()) <= {0, 1}]\n",
        "\n",
        "# 3) 연속형(클리핑 대상) 컬럼 = 숫자형 - 이진 컬럼\n",
        "continuous_cols = [col for col in numeric_cols if col not in binary_cols]\n",
        "print(continuous_cols)"
      ],
      "metadata": {
        "id": "I3JLylLPWuWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 각 컬럼별 1%, 99% 분위수 계산\n",
        "lower = df_processed[numeric_cols].quantile(0.01)\n",
        "upper = df_processed[numeric_cols].quantile(0.99)\n",
        "\n",
        "# 컬럼별로 상·하위 1% 바깥값을 클리핑\n",
        "df_processed[numeric_cols] = df_processed[numeric_cols].clip(lower=lower, upper=upper, axis=1)\n"
      ],
      "metadata": {
        "id": "xET1YZOMWt3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train/Test Split (Maintain the train/test split ratio at 8:2)\n",
        "\n",
        "Split data into training and test sets.\n",
        "\n",
        "**Split ratio:** 80% train, 20% test\n",
        "\n",
        "**Important:**\n",
        "- `stratify=y` maintains class balance in both sets\n",
        "- Test set simulates unseen data\n",
        "- **NEVER** TRAIN ON THE TEST DATA"
      ],
      "metadata": {
        "id": "Vsi59SNv2jUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Test Split\n",
        "print(\"=\"*80)\n",
        "print(\"TRAIN/TEST SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_processed.drop('Heart Attack Risk', axis=1)\n",
        "y = df_processed['Heart Attack Risk']\n",
        "\n",
        "# Split into train and test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n  Data split completed:\")\n",
        "print(f\"  Training samples:   {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test samples:       {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"\\n  Train label distribution:\")\n",
        "print(f\"    No Risk (0): {sum(y_train==0)} ({sum(y_train==0)/len(y_train)*100:.1f}%)\")\n",
        "print(f\"    Risk (1):    {sum(y_train==1)} ({sum(y_train==1)/len(y_train)*100:.1f}%)\")\n",
        "print(f\"\\n  Test label distribution:\")\n",
        "print(f\"    No Risk (0): {sum(y_test==0)} ({sum(y_test==0)/len(y_test)*100:.1f}%)\")\n",
        "print(f\"    Risk (1):    {sum(y_test==1)} ({sum(y_test==1)/len(y_test)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "VIkN3pMWy6v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Feature engineering (TODO)\n",
        "\n",
        "Feature engineering is the process of creating new features or transforming existing features to improve model performance. This includes encoding categorical variables, creating interaction features, and feature scaling.\n",
        "\n",
        "### The following is an example that requires feature engineering.\n",
        "**1. Binary Variables (2 categories)**\n",
        "- Example: Sex (Male/Female) etc.\n",
        "\n",
        "**2. Nominal Variables (no inherent order)**\n",
        "- Examples: Diet, Country, Continent, Hemisphere etc.\n"
      ],
      "metadata": {
        "id": "eAryteYuSwbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Age --> 고령자/저령 으로 구분\n",
        "2. Cholesterol --> 고 콜레스테롤 환자 구분\n",
        "3. Heart Rate --> 고혈압 환자 구분\n",
        "4. Diabetes --> 당뇨(이미 0/1로 구분되어있음)\n",
        "5. Family History -->0/1\n",
        "6. Smoking --> 0/1\n",
        "7. Obesity --> 0/1\n",
        "8. Alcohol Consumption -> 0/1\n",
        "9. Exercise Hours Per Week --> 운동 자주하는 사람 구분\n",
        "10. Previous Heart Problems\n",
        "11. Medication Use\n",
        "12. Stress Level --> 고스트레스 위험군 구분\n",
        "13. Sedentary Hours Per Day --> 앉아서 오래 생활하는지 구분\n",
        "14. Income --> 고수익자 구분\n",
        "15. BMI --> BMI 지수에 따라 구분\n",
        "16. Triglycerides\n",
        "17. Physical Activity Days Per Week\n",
        "18. Sleep Hours Per Day\n",
        "19. Heart Attack Risk\n",
        "20. Systolic_BP\n",
        "21. Diastolic_BP\n",
        "22. Sex_Male\n",
        "23. Diet_Healthy\n",
        "24. Diet_Unhealthy\n",
        "25. Country_Australia\n",
        "26. Country_Brazil\n",
        "27. Country_Canada\n",
        "28. Country_China\n",
        "29. Country_Colombia\n",
        "30. Country_France\n",
        "31. Country_Germany\n",
        "32. Country_India\n",
        "33. Country_Italy\n",
        "34. Country_Japan\n",
        "35. Country_New Zealand\n",
        "36. Country_Nigeria\n",
        "37. Country_South Africa\n",
        "38. Country_South Korea\n",
        "39. Country_Spain\n",
        "40. Country_Thailand\n",
        "41. Country_United Kingdom\n",
        "42. Country_United States\n",
        "43. Country_Vietnam\n",
        "44. Continent_Asia\n",
        "45. Continent_Australia\n",
        "46. Continent_Europe\n",
        "47. Continent_North America\n",
        "48. Continent_South America\n",
        "49. Hemisphere_Southern Hemisphere\n"
      ],
      "metadata": {
        "id": "_T-SwzMu0Waa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def add_engineered_features(X):\n",
        "    \"\"\"X: DataFrame (train 또는 test)\n",
        "       리턴: 새로운 파생 피처가 추가된 DataFrame\n",
        "    \"\"\"\n",
        "    X = X.copy()\n",
        "\n",
        "    # 1) 고령 여부\n",
        "    X['Is_Senior'] = (X['Age'] >= 65).astype(int)\n",
        "\n",
        "    # 2) 고콜레스테롤 여부 (대략 240 이상)\n",
        "    X['High_Chol'] = (X['Cholesterol'] >= 240).astype(int)\n",
        "\n",
        "    # 3) 고혈압 여부\n",
        "    X['High_BP'] = ((X['Systolic_BP'] >= 140) | (X['Diastolic_BP'] >= 90)).astype(int)\n",
        "\n",
        "    # 4) 활동 점수 & 좌식 대비 활동 비율\n",
        "    X['Exercise_Frequency'] = (X['Exercise Hours Per Week'] * X['Physical Activity Days Per Week']).astype(int)\n",
        "    X['Sedentary_per_Activity'] = X['Sedentary Hours Per Day'] / (X['Exercise_Frequency'] + 1e-3)\n",
        "\n",
        "    # 5) 스트레스 × 좌식시간\n",
        "    X['Stress_Sedentary'] = X['Stress Level'] * X['Sedentary Hours Per Day']\n",
        "\n",
        "    # 6) BMI 따라 구분\n",
        "    X['BMI_Fat'] = (X[\"BMI\"]>30).astype(int)\n",
        "\n",
        "    # 7) 고수익자일수록 관리를 잘 할 수 있는데 비만이라는건 특히나 심혈관 질환 위험이 높을 수 있다고 생각\n",
        "    X['HighIncome_Obesity'] = (X['Income'] > 200000).astype(int) * X['BMI_Fat']\n",
        "\n",
        "    # 8) BMI와 나이 결합\n",
        "    X['Age_BMI_Interaction'] = X['Age'] * X['BMI']\n",
        "\n",
        "    # 9) 운동을 자주하는데도 고혈압인 사람\n",
        "    X['Exercise_HighBP'] = X['Exercise_Frequency'] * X['High_BP']\n",
        "\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "5TEUPXq6Sp1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fe = add_engineered_features(X_train)\n",
        "X_test_fe  = add_engineered_features(X_test)\n",
        "\n",
        "print(X_train_fe.columns)"
      ],
      "metadata": {
        "id": "-9FhCfpXn2Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_feature_types(df):\n",
        "    \"\"\"\n",
        "    This function checks whether the features are binary (0/1) or continuous variables.\n",
        "\n",
        "    Args:\n",
        "    - df: DataFrame with the features to check.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing lists of binary and continuous feature names.\n",
        "    \"\"\"\n",
        "    binary_features = []    # List to store binary features (0/1)\n",
        "    continuous_features = []  # List to store continuous features (numeric)\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Check if the feature contains only 0 and 1 (binary)\n",
        "        if df[col].dtype in ['int64', 'bool'] and df[col].nunique() == 2:\n",
        "            binary_features.append(col)\n",
        "        # Check if the feature is a continuous variable (int or float)\n",
        "        elif df[col].dtype in ['int64', 'float64']:\n",
        "            continuous_features.append(col)\n",
        "\n",
        "    print(f\"Binary Features (0/1): {binary_features}\")\n",
        "    print(f\"Continuous Features: {continuous_features}\")\n",
        "\n",
        "    return binary_features, continuous_features\n",
        "\n",
        "# Example usage with df_processed\n",
        "binary_features, continuous_features = check_feature_types(X_train_fe)\n"
      ],
      "metadata": {
        "id": "6zWPS2qWycYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_fe[continuous_features] = scaler.fit_transform(X_train_fe[continuous_features])\n",
        "X_test_fe[continuous_features] = scaler.transform(X_test_fe[continuous_features])\n",
        "\n",
        "print(X_train_fe[continuous_features].head())"
      ],
      "metadata": {
        "id": "Jj5zhmoPyqx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Model Evaluation Functions (This is just an example. You may modify this if you want.)\n",
        "\n",
        "Pre-built functions to evaluate your models consistently.\n",
        "\n",
        "**Functions provided:**\n",
        "- `evaluate_model()`: Calculate accuracy, precision, recall, F1-score\n",
        "- `plot_confusion_matrix()`: Visualize prediction errors\n",
        "\n",
        "**Evaluation metrics explained:**\n",
        "- **Accuracy**: Overall correctness\n",
        "- **Precision**: Of predicted positives, how many are correct?\n",
        "- **Recall**: Of actual positives, how many did we find?\n",
        "- **F1-Score**: Harmonic mean of precision and recall"
      ],
      "metadata": {
        "id": "WgmUme3f2lS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Model Evaluation Functions\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL EVALUATION FUNCTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Evaluate model performance with multiple metrics\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        model_name: Name of the model for display\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing all metrics\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(f\"\\n  Confusion Matrix:\")\n",
        "    print(f\"    TN={cm[0,0]:4d}  FP={cm[0,1]:4d}\")\n",
        "    print(f\"    FN={cm[1,0]:4d}  TP={cm[1,1]:4d}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrix(cm, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix as heatmap\n",
        "\n",
        "    Args:\n",
        "        cm: Confusion matrix\n",
        "        model_name: Name of the model for title\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['No Risk', 'Risk'],\n",
        "                yticklabels=['No Risk', 'Risk'])\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"  Evaluation functions defined:\")\n",
        "print(\"  - evaluate_model(y_true, y_pred, model_name)\")\n",
        "print(\"  - plot_confusion_matrix(cm, model_name)\")"
      ],
      "metadata": {
        "id": "BBaaEwDFzF0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Memory Measurement Functions (This is just an example. You may modify this if you want.)\n",
        "\n",
        "For Task 2, you need to minimize model size.\n",
        "\n",
        "**Functions provided:**\n",
        "- `measure_model_memory()`: Get model size\n",
        "- `evaluate_model_with_memory()`: Combined performance and memory evaluation"
      ],
      "metadata": {
        "id": "hiOswCr42oe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory Measurement Functions\n",
        "print(\"=\"*80)\n",
        "print(\"MEMORY MEASUREMENT FUNCTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def measure_model_memory(model):\n",
        "    \"\"\"\n",
        "    Measure model size in memory\n",
        "\n",
        "    Args:\n",
        "        model: Trained model object\n",
        "\n",
        "    Returns:\n",
        "        Model size\n",
        "    \"\"\"\n",
        "    model_size = sys.getsizeof(pickle.dumps(model)) / 1024  # Model size in KB\n",
        "    return model_size\n",
        "\n",
        "def evaluate_model_with_memory(model, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Evaluate model performance and memory usage\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        X_test: Test features\n",
        "        y_test: Test labels\n",
        "        model_name: Name of the model\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with metrics and memory info\n",
        "    \"\"\"\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Performance metrics\n",
        "    metrics = evaluate_model(y_test, y_pred, model_name)\n",
        "\n",
        "    # Memory measurement\n",
        "    memory = measure_model_memory(model)\n",
        "\n",
        "    metrics['memory'] = memory\n",
        "\n",
        "    return metrics\n",
        "\n",
        "print(\"  Memory measurement functions defined:\")\n",
        "print(\"  - measure_model_memory(model)\")\n",
        "print(\"  - evaluate_model_with_memory(model, X_test, y_test, model_name)\")"
      ],
      "metadata": {
        "id": "uCctCBN6zQgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 코드로 형식 지정됨\n",
        "```\n",
        "\n",
        "## 10. Model Implementation (TODO) (This is just an example. You may modify this if you want.)\n",
        "\n",
        "**This is where you implement your models**\n",
        "\n",
        "**Tips:**\n",
        "- Start with simple models\n",
        "- Gradually increase complexity"
      ],
      "metadata": {
        "id": "QsjSPZ2v2s3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Implement your models here\n",
        "\n",
        "Task 1: Best Prediction Performance\n",
        "- Goal: Achieve the highest accuracy possible\n",
        "- Suggested approaches:\n",
        "  * Try feature engineering\n",
        "  * Experiment with different hyperparameters\n",
        "\n",
        "Task 2: Minimize Memory with Accuracy >= 60%\n",
        "- Goal: Smallest model size while maintaining at least 60% accuracy\n",
        "- Suggested approaches:\n",
        "  * Try feature selection\n",
        "\n",
        "Task 3: Best Performance without Neural Networks\n",
        "- Goal: Highest accuracy using classical ML algorithms only\n",
        "- Suggested approaches:\n",
        "  * Feature engineering and selection\n",
        "\n",
        "Example Model Implementation Structure:\n",
        "\n",
        "\n",
        "TODO: Create and train your models here\n",
        "Example:\n",
        "model_task1 = YourBestModel()\n",
        "model_task1.fit(X_train, y_train)\n",
        "\n",
        "TODO: For Task 2\n",
        "model_task2 = YourMemoryEfficientModel()\n",
        "model_task2.fit(X_train, y_train)\n",
        "\n",
        "TODO: For Task 3, implement non-neural network models\n",
        "model_task3 = YourBestNonNNModel()\n",
        "model_task3.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "yDh2RuivbwA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test_fe.shape)"
      ],
      "metadata": {
        "id": "nS7okzBWdQ6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pca_features(X_train_fe, X_test_fe, n_components=20):\n",
        "    X_train_np = X_train_fe.values.astype(float)\n",
        "    X_test_np = X_test_fe.values.astype(float)\n",
        "\n",
        "    # 1) centering\n",
        "    mean = X_train_np.mean(axis=0, keepdims=True)\n",
        "    X_train_centered = X_train_np - mean\n",
        "    X_test_centered = X_test_np - mean\n",
        "\n",
        "    # 2) covariance\n",
        "    cov = np.cov(X_train_centered, rowvar=False)\n",
        "\n",
        "    # 3) eigen decomposition\n",
        "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
        "\n",
        "    # 4) sort by eigenvalue (desc)\n",
        "    idx = np.argsort(eigvals)[::-1]\n",
        "    eigvals = eigvals[idx]\n",
        "    eigvecs = eigvecs[:, idx]\n",
        "\n",
        "    # 5) explained variance ratio\n",
        "    explained_variance_ratio = eigvals / eigvals.sum()\n",
        "\n",
        "    # 6) top n_components만 사용\n",
        "    eigvecs_top = eigvecs[:, :n_components]\n",
        "    evr_top = explained_variance_ratio[:n_components]\n",
        "\n",
        "    X_train_pca_np = X_train_centered @ eigvecs_top\n",
        "    X_test_pca_np = X_test_centered @ eigvecs_top\n",
        "\n",
        "    pc_cols = [f\"PC{i+1}\" for i in range(n_components)]\n",
        "    X_train_pca = pd.DataFrame(X_train_pca_np, columns=pc_cols, index=X_train_fe.index)\n",
        "    X_test_pca = pd.DataFrame(X_test_pca_np, columns=pc_cols, index=X_test_fe.index)\n",
        "\n",
        "    # 로딩 행렬: row = 원래 feature, col = PC\n",
        "    loadings = pd.DataFrame(\n",
        "        eigvecs_top,\n",
        "        index=X_train_fe.columns,\n",
        "        columns=pc_cols,\n",
        "    )\n",
        "\n",
        "    return X_train_pca, X_test_pca, loadings, evr_top"
      ],
      "metadata": {
        "id": "Z9PluRXcJ-DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect # Import inspect for get_params\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class LogisticRegressionScratch(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    간단한 L2-정규화 로지스틱 회귀 (배치 GD)\n",
        "    개선: 가중치 초기화, 클래스 가중치, 학습률 스케줄링\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.1, epochs=1000, reg_lambda=0.01, class_weight=None, threshold=0.5, random_state=None):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.class_weight = class_weight  # None or 'balanced' or dict\n",
        "        self.threshold = threshold\n",
        "        self.random_state = random_state # Added random_state\n",
        "        self.w = None\n",
        "        self.b = 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def _sigmoid(z):\n",
        "        # 수치 안정성을 위해 클리핑\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.random_state is not None:\n",
        "            np.random.seed(self.random_state)\n",
        "\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y).reshape(-1, 1)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 가중치 초기화 개선 (Xavier 초기화)\n",
        "        self.w = np.random.normal(0, 0.01, (n_features, 1))\n",
        "        self.b = 0.0\n",
        "\n",
        "        # 클래스 가중치 계산\n",
        "        sample_weights = np.ones(n_samples)\n",
        "        if self.class_weight == 'balanced':\n",
        "            n_pos = np.sum(y == 1)\n",
        "            n_neg = np.sum(y == 0)\n",
        "            if n_pos > 0 and n_neg > 0:\n",
        "                sample_weights[y.ravel() == 0] = n_samples / (2 * n_neg)\n",
        "                sample_weights[y.ravel() == 1] = n_samples / (2 * n_pos)\n",
        "        elif isinstance(self.class_weight, dict):\n",
        "            sample_weights[y.ravel() == 0] = self.class_weight.get(0, 1.0)\n",
        "            sample_weights[y.ravel() == 1] = self.class_weight.get(1, 1.0)\n",
        "\n",
        "        sample_weights = sample_weights.reshape(-1, 1)\n",
        "\n",
        "        # 학습률 스케줄링을 위한 초기 학습률 저장\n",
        "        initial_lr = self.lr\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            z = X @ self.w + self.b\n",
        "            y_hat = self._sigmoid(z)\n",
        "\n",
        "            # 가중치가 적용된 손실의 그래디언트\n",
        "            error = (y_hat - y) * sample_weights\n",
        "            grad_w = (X.T @ error) / n_samples + self.reg_lambda * self.w / n_samples\n",
        "            grad_b = np.mean(error)\n",
        "\n",
        "            # 학습률 감소 (선형 스케줄링)\n",
        "            current_lr = initial_lr * (1 - epoch / self.epochs) * 0.5 + initial_lr * 0.5\n",
        "\n",
        "            self.w -= current_lr * grad_w\n",
        "            self.b -= current_lr * grad_b\n",
        "        return self # Return self for sklearn compatibility\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.asarray(X)\n",
        "        return self._sigmoid(X @ self.w + self.b).ravel()\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= self.threshold).astype(int)\n",
        "\n",
        "    # Implement get_params and set_params for sklearn compatibility\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'lr': self.lr,\n",
        "            'epochs': self.epochs,\n",
        "            'reg_lambda': self.reg_lambda,\n",
        "            'class_weight': self.class_weight,\n",
        "            'threshold': self.threshold,\n",
        "            'random_state': self.random_state\n",
        "        }\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "class DecisionTreeNode:\n",
        "    def __init__(self, gini, num_samples, num_pos, prediction, random_state=42):\n",
        "        self.gini = gini\n",
        "        self.num_samples = num_samples\n",
        "        self.num_pos = num_pos\n",
        "        self.prediction = prediction\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "\n",
        "\n",
        "class DecisionTreeClassifierScratch(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    CART 결정트리 (gini 기준)\n",
        "    개선: 클래스 가중치, min_samples_leaf 추가\n",
        "    \"\"\"\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
        "                 max_features=None, class_weight=None, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features  # None → 모든 피처\n",
        "        self.class_weight = class_weight  # None or 'balanced' or dict\n",
        "        self.random_state = random_state # Added random_state\n",
        "        self.n_features_ = None\n",
        "        self.root = None\n",
        "\n",
        "    def _gini(self, y, sample_weights=None):\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        if sample_weights is None:\n",
        "            p = np.mean(y)\n",
        "        else:\n",
        "            total_weight = np.sum(sample_weights)\n",
        "            if total_weight == 0:\n",
        "                return 0\n",
        "            p = np.sum(y * sample_weights) / total_weight\n",
        "        return 2 * p * (1 - p)\n",
        "\n",
        "    def _best_split(self, X, y, feature_indices, sample_weights=None):\n",
        "        m = y.size\n",
        "        if m < self.min_samples_split:\n",
        "            return None, None\n",
        "\n",
        "        best_gini = 1.0\n",
        "        best_idx, best_thr = None, None\n",
        "\n",
        "        for idx in feature_indices:\n",
        "            x_sorted = X[:, idx].argsort()\n",
        "            X_i = X[x_sorted, idx]\n",
        "            y_i = y[x_sorted]\n",
        "            w_i = sample_weights[x_sorted] if sample_weights is not None else None\n",
        "\n",
        "            # 후보 임계값: 인접 값 평균\n",
        "            unique_vals = np.unique(X_i)\n",
        "            if unique_vals.size == 1:\n",
        "                continue\n",
        "            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2\n",
        "\n",
        "            left_count = 0\n",
        "            left_pos = 0\n",
        "            left_weight = 0\n",
        "            right_count = m\n",
        "            right_pos = np.sum(y_i)\n",
        "            right_weight = np.sum(w_i) if w_i is not None else m\n",
        "\n",
        "            j = 0\n",
        "            for thr in thresholds:\n",
        "                while j < m and X_i[j] <= thr:\n",
        "                    left_count += 1\n",
        "                    left_pos += y_i[j]\n",
        "                    if w_i is not None:\n",
        "                        left_weight += w_i[j]\n",
        "                        right_weight -= w_i[j]\n",
        "                    right_count -= 1\n",
        "                    right_pos -= y_i[j]\n",
        "                    j += 1\n",
        "\n",
        "                if left_count < self.min_samples_leaf or right_count < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                gini_left = self._gini(y_i[:left_count], w_i[:left_count] if w_i is not None else None)\n",
        "                gini_right = self._gini(y_i[left_count:], w_i[left_count:] if w_i is not None else None)\n",
        "\n",
        "                if w_i is not None:\n",
        "                    total_weight = left_weight + right_weight\n",
        "                    if total_weight == 0:\n",
        "                        continue\n",
        "                    gini = (left_weight * gini_left + right_weight * gini_right) / total_weight\n",
        "                else:\n",
        "                    gini = (left_count * gini_left + right_count * gini_right) / m\n",
        "\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_idx = idx\n",
        "                    best_thr = thr\n",
        "\n",
        "        return best_idx, best_thr\n",
        "\n",
        "    def _build(self, X, y, depth, sample_weights=None):\n",
        "        num_samples = y.size\n",
        "        if sample_weights is None:\n",
        "            num_pos = np.sum(y)\n",
        "            total_weight = num_samples\n",
        "        else:\n",
        "            num_pos = np.sum(y * sample_weights)\n",
        "            total_weight = np.sum(sample_weights)\n",
        "\n",
        "        node = DecisionTreeNode(\n",
        "            gini=self._gini(y, sample_weights),\n",
        "            num_samples=num_samples,\n",
        "            num_pos=num_pos,\n",
        "            prediction=1 if num_pos >= total_weight / 2 else 0\n",
        "        )\n",
        "\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            return node\n",
        "        if num_samples < self.min_samples_split or node.gini == 0.0:\n",
        "            return node\n",
        "\n",
        "        if self.max_features is None:\n",
        "            feat_idx = np.arange(self.n_features_)\n",
        "        else:\n",
        "            # Use self.random_state for reproducible feature selection\n",
        "            rng = np.random.RandomState(self.random_state + depth if self.random_state is not None else None)\n",
        "            feat_idx = rng.choice(self.n_features_, self.max_features, replace=False)\n",
        "\n",
        "        idx, thr = self._best_split(X, y, feat_idx, sample_weights)\n",
        "        if idx is None:\n",
        "            return node\n",
        "\n",
        "        indices_left = X[:, idx] <= thr\n",
        "        X_left, y_left = X[indices_left], y[indices_left]\n",
        "        X_right, y_right = X[~indices_left], y[~indices_left]\n",
        "        w_left = sample_weights[indices_left] if sample_weights is not None else None\n",
        "        w_right = sample_weights[~indices_left] if sample_weights is not None else None\n",
        "\n",
        "        node.feature_index = idx\n",
        "        node.threshold = thr\n",
        "        node.left = self._build(X_left, y_left, depth + 1, w_left)\n",
        "        node.right = self._build(X_right, y_right, depth + 1, w_right)\n",
        "        return node\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.random_state is not None:\n",
        "            np.random.seed(self.random_state)\n",
        "\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        self.n_features_ = X.shape[1]\n",
        "\n",
        "        # 클래스 가중치 계산\n",
        "        sample_weights = None\n",
        "        if self.class_weight == 'balanced':\n",
        "            n_pos = np.sum(y == 1)\n",
        "            n_neg = np.sum(y == 0)\n",
        "            n_samples = len(y)\n",
        "            if n_pos > 0 and n_neg > 0:\n",
        "                sample_weights = np.ones(n_samples)\n",
        "                sample_weights[y == 0] = n_samples / (2 * n_neg)\n",
        "                sample_weights[y == 1] = n_samples / (2 * n_pos)\n",
        "        elif isinstance(self.class_weight, dict):\n",
        "            sample_weights = np.ones(len(y))\n",
        "            sample_weights[y == 0] = self.class_weight.get(0, 1.0)\n",
        "            sample_weights[y == 1] = self.class_weight.get(1, 1.0)\n",
        "\n",
        "        self.root = self._build(X, y, depth=0, sample_weights=sample_weights)\n",
        "        return self # Return self for sklearn compatibility\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        if node.feature_index is None:\n",
        "            return node.prediction\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._predict_one(x, node.left)\n",
        "        else:\n",
        "            return self._predict_one(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        return np.array([self._predict_one(x, self.root) for x in X])\n",
        "\n",
        "    # Implement get_params and set_params for sklearn compatibility\n",
        "    def get_params(self, deep=True):\n",
        "        # Get all parameters from the constructor\n",
        "        params = inspect.signature(self.__init__).parameters\n",
        "        return {param: getattr(self, param) for param in params if param != 'self'}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "\n",
        "class RandomForestClassifierScratch(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    간단한 랜덤 포레스트 (bagging + 랜덤 피처)\n",
        "    개선: 클래스 가중치, min_samples_leaf 추가\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=20, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
        "                 max_features='sqrt', bootstrap=True, class_weight=None, random_state=42):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features  # 'sqrt' or int or None\n",
        "        self.bootstrap = bootstrap\n",
        "        self.class_weight = class_weight\n",
        "        self.random_state = random_state\n",
        "        self.trees = []\n",
        "\n",
        "    def _get_max_features(self, n_features):\n",
        "        if self.max_features == 'sqrt':\n",
        "            return max(1, int(np.sqrt(n_features)))\n",
        "        if isinstance(self.max_features, int):\n",
        "            return min(n_features, self.max_features)\n",
        "        return n_features\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        n_samples, n_features = X.shape\n",
        "        m_features = self._get_max_features(n_features)\n",
        "\n",
        "        self.trees = []\n",
        "        for i in range(self.n_estimators):\n",
        "            if self.bootstrap:\n",
        "                indices = rng.randint(0, n_samples, n_samples)\n",
        "            else:\n",
        "                indices = np.arange(n_samples)\n",
        "            X_sample = X[indices]\n",
        "            y_sample = y[indices]\n",
        "\n",
        "            # Pass random_state to DecisionTreeClassifierScratch for reproducibility\n",
        "            tree_random_state = self.random_state + i if self.random_state is not None else None\n",
        "\n",
        "            tree = DecisionTreeClassifierScratch(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_samples_leaf=self.min_samples_leaf,\n",
        "                max_features=m_features,\n",
        "                class_weight=self.class_weight,\n",
        "                random_state=tree_random_state\n",
        "            )\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "        return self # Return self for sklearn compatibility\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        # 다수결\n",
        "        votes = np.mean(preds, axis=0)\n",
        "        return (votes >= 0.5).astype(int)\n",
        "\n",
        "    # Implement get_params and set_params for sklearn compatibility\n",
        "    def get_params(self, deep=True):\n",
        "        # Get all parameters from the constructor\n",
        "        params = inspect.signature(self.__init__).parameters\n",
        "        return {param: getattr(self, param) for param in params if param != 'self'}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self"
      ],
      "metadata": {
        "id": "rCC-HPoCa6bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Model Evaluation (TODO) (This is just an example. You may modify this if you want.)\n",
        "\n",
        "After implementing your models, evaluate them here.\n",
        "\n",
        "**What to do:**\n",
        "1. Train each model on training data\n",
        "2. Evaluate on test data using provided functions\n",
        "3. Plot confusion matrices\n",
        "4. Compare results across tasks\n",
        "\n",
        "**Task-specific checks:**\n",
        "- Task 2: Verify accuracy ≥ 60%\n",
        "- Task 3: Confirm no neural networks used"
      ],
      "metadata": {
        "id": "2NXN1YIM2xkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DEFINING HYPERPARAMETER GRIDS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Parameter Grid for Logistic Regression\n",
        "logreg_param_grid = {\n",
        "    'lr': [0.01, 0.05, 0.1],\n",
        "    'epochs': [1000, 1500],\n",
        "    'reg_lambda': [0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "# Parameter Grid for Decision Tree\n",
        "tree_param_grid = {\n",
        "    'max_depth': [10, 15, 20, None],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Parameter Grid for Random Forest (reduced for quicker execution, can be expanded)\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [20, 30, 40],\n",
        "    'max_depth': [20, None],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "print(\"Parameter grids defined for Logistic Regression, Decision Tree, and Random Forest.\")"
      ],
      "metadata": {
        "id": "aZgX2gfYiuH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EXECUTING GRID SEARCH FOR HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- Logistic Regression GridSearchCV ---\n",
        "print(\"\\n1. Running GridSearchCV for Logistic Regression...\")\n",
        "logreg_grid_search = GridSearchCV(\n",
        "    estimator=LogisticRegressionScratch(class_weight='balanced', random_state=RANDOM_SEED),\n",
        "    param_grid=logreg_param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3, # Using 3-fold cross-validation for speed\n",
        "    n_jobs=-1, # Use all available cores\n",
        "    verbose=1\n",
        ")\n",
        "logreg_grid_search.fit(X_train_fe, y_train)\n",
        "\n",
        "print(\"   Best parameters for Logistic Regression:\", logreg_grid_search.best_params_)\n",
        "print(\"   Best F1 score for Logistic Regression:\", logreg_grid_search.best_score_)\n",
        "\n",
        "# --- Decision Tree GridSearchCV ---\n",
        "print(\"\\n2. Running GridSearchCV for Decision Tree...\")\n",
        "tree_grid_search = GridSearchCV(\n",
        "    estimator=DecisionTreeClassifierScratch(class_weight='balanced', random_state=RANDOM_SEED),\n",
        "    param_grid=tree_param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "tree_grid_search.fit(X_train_fe, y_train)\n",
        "\n",
        "print(\"   Best parameters for Decision Tree:\", tree_grid_search.best_params_)\n",
        "print(\"   Best F1 score for Decision Tree:\", tree_grid_search.best_score_)\n",
        "\n",
        "# --- Random Forest GridSearchCV ---\n",
        "print(\"\\n3. Running GridSearchCV for Random Forest...\")\n",
        "rf_grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifierScratch(class_weight='balanced', random_state=RANDOM_SEED),\n",
        "    param_grid=rf_param_grid,\n",
        "    scoring='f1',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "rf_grid_search.fit(X_train_fe, y_train)\n",
        "\n",
        "print(\"   Best parameters for Random Forest:\", rf_grid_search.best_params_)\n",
        "print(\"   Best F1 score for Random Forest:\", rf_grid_search.best_score_)\n",
        "\n",
        "print(\"\\nGrid search completed for all models.\")"
      ],
      "metadata": {
        "id": "2jJNSZaJi9qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EVALUATING TUNED MODELS ON TEST DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results={}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TASK 1: BEST PREDICTION PERFORMANCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "metrics_rf_tuned = evaluate_model_with_memory(best_rf_model, X_test_fe, y_test, \"RF-Tuned\")\n",
        "results[\"RF-Tuned\"] = metrics_rf_tuned\n",
        "plot_confusion_matrix(metrics_rf_tuned['confusion_matrix'], \"Task 1 Model\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TASK 2: MEMORY EFFICIENCY (Accuracy >= 60%)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_logreg_model = logreg_grid_search.best_estimator_\n",
        "metrics_logreg_tuned = evaluate_model_with_memory(best_logreg_model, X_test_fe, y_test, \"LogReg-Tuned\")\n",
        "results[\"LogReg-Tuned\"] = metrics_logreg_tuned\n",
        "plot_confusion_matrix(metrics_logreg_tuned['confusion_matrix'], \"Task 2 Model\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TASK 3: BEST PERFORMANCE WITHOUT NEURAL NETWORKS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "best_tree_model = tree_grid_search.best_estimator_\n",
        "metrics_tree_tuned = evaluate_model_with_memory(best_tree_model, X_test_fe, y_test, \"Tree-Tuned\")\n",
        "results[\"Tree-Tuned\"] = metrics_tree_tuned\n",
        "plot_confusion_matrix(metrics_tree_tuned['confusion_matrix'], \"Task 3 Model\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nEvaluation of tuned models completed.\")"
      ],
      "metadata": {
        "id": "6C44qiCkixUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Error Analysis (This is just an example. You may modify this if you want.)\n",
        "\n",
        "Understand where and why your models fail.\n",
        "\n",
        "**Analysis includes:**\n",
        "- False Positives: Predicted risk, but no actual risk\n",
        "- False Negatives: Predicted no risk, but actual risk"
      ],
      "metadata": {
        "id": "E4xRBAc32zp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test.shape)\n",
        "print(X_train_fe.shape)"
      ],
      "metadata": {
        "id": "yz5OttClgXeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Error Analysis\n",
        "print(\"=\"*80)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\"\"\"\n",
        "Analyze where your model makes mistakes\n",
        "This helps you understand model limitations and potential improvements\n",
        "\"\"\"\n",
        "\n",
        "def perform_error_analysis(model, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Perform detailed error analysis\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        X_test: Test features (numpy array or DataFrame)\n",
        "        y_test: True labels\n",
        "        model_name: Name of the model\n",
        "    \"\"\"\n",
        "    print(f\"\\nError Analysis for {model_name}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Convert to DataFrame if numpy array\n",
        "    if isinstance(X_test, np.ndarray):\n",
        "        X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
        "    else:\n",
        "        X_test_df = X_test\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Identify errors\n",
        "    errors = y_pred != y_test\n",
        "    false_positives = (y_pred == 1) & (y_test == 0)\n",
        "    false_negatives = (y_pred == 0) & (y_test == 1)\n",
        "\n",
        "    print(f\"\\nError Summary:\")\n",
        "    print(f\"  Total errors: {sum(errors)} ({sum(errors)/len(y_test)*100:.2f}%)\")\n",
        "    print(f\"  False Positives: {sum(false_positives)} (predicted Risk, actually No Risk)\")\n",
        "    print(f\"  False Negatives: {sum(false_negatives)} (predicted No Risk, actually Risk)\")\n",
        "\n",
        "    # Analyze false positives\n",
        "    if sum(false_positives) > 0:\n",
        "        print(f\"\\nFalse Positive Analysis:\")\n",
        "        fp_data = X_test_df[false_positives]\n",
        "        print(f\"  Average feature values for False Positives:\")\n",
        "        print(fp_data.mean())\n",
        "\n",
        "    # Analyze false negatives\n",
        "    if sum(false_negatives) > 0:\n",
        "        print(f\"\\nFalse Negative Analysis:\")\n",
        "        fn_data = X_test_df[false_negatives]\n",
        "        print(f\"  Average feature values for False Negatives:\")\n",
        "        print(fn_data.mean())\n",
        "\n",
        "    return {\n",
        "        'false_positives': sum(false_positives),\n",
        "        'false_negatives': sum(false_negatives),\n",
        "        'total_errors': sum(errors)\n",
        "    }\n",
        "\n",
        "\n",
        "error_analysis_task1 = perform_error_analysis(\n",
        "    model_task1, X_test_fe, y_test, \"Task 1 Model\"\n",
        ")\n",
        "\n",
        "error_analysis_task2 = perform_error_analysis(\n",
        "    model_task2, X_test_fe, y_test, \"Task 2 Model\"\n",
        ")\n",
        "\n",
        "error_analysis_task3 = perform_error_analysis(\n",
        "    model_task3, X_test_fe, y_test, \"Task 3 Model\"\n",
        ")"
      ],
      "metadata": {
        "id": "IkFOXNwGzj_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Clustering Visualization (This is just an example. You may modify this if you want.)\n",
        "\n",
        "In this section, you will implement K-means clustering from scratch and visualize the data using Principal Component Analysis (PCA).\n",
        "\n",
        "**Goals:**\n",
        "- Implement K-means clustering algorithm\n",
        "- Implement PCA for dimensionality reduction\n",
        "- Reduce dimensionality to 2D for visualization\n",
        "- Understand natural groupings in the data\n",
        "- Analyze the relationship between clusters and heart attack risk\n",
        "\n",
        "**What you need to implement:**\n",
        "- K-means clustering algorithm (initialization, assignment, update steps)\n",
        "- PCA for dimensionality reduction (covariance matrix, eigenvalues/eigenvectors)\n",
        "- Cluster visualization"
      ],
      "metadata": {
        "id": "bhnPwpVO22Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Clustering Visualization (Student Implementation)\n",
        "print(\"=\"*80)\n",
        "print(\"CLUSTERING VISUALIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\"\"\"\n",
        "TODO: Implement K-means clustering and PCA from scratch\n",
        "\n",
        "You need to implement the following:\n",
        "1. PCA (Principal Component Analysis)\n",
        "   - Compute covariance matrix\n",
        "   - Calculate eigenvalues and eigenvectors\n",
        "   - Project data onto principal components\n",
        "2. K-means clustering algorithm\n",
        "   - Cluster initialization\n",
        "   - Cluster assignment\n",
        "   - Centroid update\n",
        "   - Convergence check\n",
        "\"\"\"\n",
        "\n",
        "class PCAImplementation:\n",
        "    \"\"\"\n",
        "    TODO: Implement PCA (Principal Component Analysis)\n",
        "\n",
        "    Your implementation should include:\n",
        "    - __init__(self, n_components): Initialize parameters\n",
        "    - fit(self, X): Fit PCA to data (compute principal components)\n",
        "    - transform(self, X): Project data onto principal components\n",
        "    - fit_transform(self, X): Fit and transform in one step\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_components=2):\n",
        "        \"\"\"\n",
        "        Initialize PCA\n",
        "\n",
        "        Args:\n",
        "            n_components: Number of principal components to keep\n",
        "        \"\"\"\n",
        "        self.n_components = n_components\n",
        "        self.mean = None\n",
        "        self.components = None  # Principal components (eigenvectors)\n",
        "        self.explained_variance_ratio = None\n",
        "\n",
        "        # TODO: Add any additional attributes you need\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Fit PCA to the data\n",
        "\n",
        "        Algorithm steps:\n",
        "        1. Center the data (subtract mean)\n",
        "        2. Compute covariance matrix\n",
        "        3. Calculate eigenvalues and eigenvectors\n",
        "        4. Sort eigenvectors by eigenvalues (descending)\n",
        "        5. Select top n_components eigenvectors\n",
        "\n",
        "        Hint: Use np.linalg.eig() for eigenvalue decomposition\n",
        "\n",
        "        Args:\n",
        "            X: Data matrix (n_samples, n_features)\n",
        "        \"\"\"\n",
        "        # TODO: Implement PCA fitting\n",
        "        # Step 1: Center the data\n",
        "        # Step 2: Compute covariance matrix\n",
        "        # Step 3: Compute eigenvalues and eigenvectors\n",
        "        # Step 4: Sort and select top components\n",
        "        # Step 5: Calculate explained variance ratio\n",
        "\n",
        "        pass\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Transform data using fitted principal components\n",
        "\n",
        "        Args:\n",
        "            X: Data matrix (n_samples, n_features)\n",
        "\n",
        "        Returns:\n",
        "            X_transformed: Projected data (n_samples, n_components)\n",
        "        \"\"\"\n",
        "        # TODO: Project data onto principal components\n",
        "        # Hint: (X - mean) @ components.T\n",
        "        pass\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"\n",
        "        Fit PCA and transform data in one step\n",
        "\n",
        "        Args:\n",
        "            X: Data matrix\n",
        "\n",
        "        Returns:\n",
        "            X_transformed: Projected data\n",
        "        \"\"\"\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "\n",
        "\n",
        "class KMeansClustering:\n",
        "    \"\"\"\n",
        "    TODO: Implement K-means clustering algorithm\n",
        "\n",
        "    Your implementation should include:\n",
        "    - __init__(self, n_clusters, max_iters, random_state): Initialize parameters\n",
        "    - fit(self, X): Fit the model to data\n",
        "    - predict(self, X): Predict cluster labels for data\n",
        "    - _initialize_centroids(self, X): Initialize cluster centroids\n",
        "    - _assign_clusters(self, X): Assign points to nearest centroid\n",
        "    - _update_centroids(self, X, labels): Update centroid positions\n",
        "    - _has_converged(self, old_centroids, new_centroids): Check convergence\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters=3, max_iters=100, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize K-means clustering\n",
        "\n",
        "        Args:\n",
        "            n_clusters: Number of clusters\n",
        "            max_iters: Maximum number of iterations\n",
        "            random_state: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iters = max_iters\n",
        "        self.random_state = random_state\n",
        "        self.centroids = None\n",
        "        self.labels_ = None\n",
        "\n",
        "        # TODO: Add any additional attributes you need\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Fit K-means to the data\n",
        "\n",
        "        Algorithm steps:\n",
        "        1. Initialize centroids randomly\n",
        "        2. Repeat until convergence or max_iters:\n",
        "           a. Assign each point to nearest centroid\n",
        "           b. Update centroids as mean of assigned points\n",
        "           c. Check for convergence\n",
        "\n",
        "        Args:\n",
        "            X: Data matrix (n_samples, n_features)\n",
        "        \"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "\n",
        "        # TODO: Implement K-means algorithm\n",
        "        # Step 1: Initialize centroids\n",
        "        # Step 2: Iterate until convergence\n",
        "        #   - Assign points to clusters\n",
        "        #   - Update centroids\n",
        "        #   - Check convergence\n",
        "\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Predict cluster labels for data\n",
        "\n",
        "        Args:\n",
        "            X: Data matrix (n_samples, n_features)\n",
        "\n",
        "        Returns:\n",
        "            labels: Cluster labels for each sample\n",
        "        \"\"\"\n",
        "        # TODO: Assign each point to nearest centroid\n",
        "        pass\n",
        "\n",
        "    def _initialize_centroids(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Initialize centroids randomly\n",
        "\n",
        "        Hint: You can randomly select n_clusters points from X as initial centroids\n",
        "\n",
        "        Args:\n",
        "            X: Data matrix\n",
        "\n",
        "        Returns:\n",
        "            centroids: Initial centroid positions\n",
        "        \"\"\"\n",
        "        # TODO: Implement centroid initialization\n",
        "        pass\n",
        "\n",
        "    def _assign_clusters(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Assign each point to the nearest centroid\n",
        "\n",
        "        Hint: Calculate Euclidean distance from each point to each centroid\n",
        "              Assign point to cluster with minimum distance\n",
        "\n",
        "        Args:\n",
        "            X: Data matrix\n",
        "\n",
        "        Returns:\n",
        "            labels: Cluster assignment for each point\n",
        "        \"\"\"\n",
        "        # TODO: Implement cluster assignment\n",
        "        pass\n",
        "\n",
        "    def _update_centroids(self, X, labels):\n",
        "        \"\"\"\n",
        "        TODO: Update centroid positions as mean of assigned points\n",
        "\n",
        "        Args:\n",
        "            X: Data matrix\n",
        "            labels: Current cluster assignments\n",
        "\n",
        "        Returns:\n",
        "            centroids: Updated centroid positions\n",
        "        \"\"\"\n",
        "        # TODO: Implement centroid update\n",
        "        pass\n",
        "\n",
        "    def _has_converged(self, old_centroids, new_centroids, tolerance=1e-4):\n",
        "        \"\"\"\n",
        "        TODO: Check if centroids have converged\n",
        "\n",
        "        Hint: Check if the change in centroid positions is below tolerance\n",
        "\n",
        "        Args:\n",
        "            old_centroids: Previous centroid positions\n",
        "            new_centroids: Current centroid positions\n",
        "            tolerance: Convergence threshold\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating convergence\n",
        "        \"\"\"\n",
        "        # TODO: Implement convergence check\n",
        "        pass\n",
        "\n",
        "\n",
        "def visualize_clusters_with_pca(X, y, n_clusters=3):\n",
        "    \"\"\"\n",
        "    Visualize data using your K-means and PCA implementations\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix (numpy array or DataFrame)\n",
        "        y: Labels (for comparison)\n",
        "        n_clusters: Number of clusters\n",
        "    \"\"\"\n",
        "    print(f\"\\nPerforming clustering visualization...\")\n",
        "    print(f\"  Number of clusters: {n_clusters}\")\n",
        "\n",
        "    # Convert to numpy array if DataFrame\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        X_array = X.values\n",
        "    else:\n",
        "        X_array = X\n",
        "\n",
        "    # TODO: Use your PCA implementation\n",
        "    # pca = PCAImplementation(n_components=2)\n",
        "    # X_pca = pca.fit_transform(X_array)\n",
        "\n",
        "    # print(f\"\\n  PCA Explained Variance Ratio:\")\n",
        "    # print(f\"    PC1: {pca.explained_variance_ratio[0]*100:.2f}%\")\n",
        "    # print(f\"    PC2: {pca.explained_variance_ratio[1]*100:.2f}%\")\n",
        "    # print(f\"    Total: {sum(pca.explained_variance_ratio)*100:.2f}%\")\n",
        "\n",
        "    # TODO: Use your K-means implementation\n",
        "    # kmeans = KMeansClustering(n_clusters=n_clusters, random_state=RANDOM_SEED)\n",
        "    # kmeans.fit(X_array)\n",
        "    # cluster_labels = kmeans.predict(X_array)\n",
        "\n",
        "    # Placeholder for visualization (uncomment after implementing K-means and PCA)\n",
        "    # Create visualization\n",
        "    # fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # # Plot 1: Colored by true labels\n",
        "    # scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y,\n",
        "    #                            cmap='coolwarm', alpha=0.6, edgecolors='k', linewidth=0.5)\n",
        "    # axes[0].set_xlabel(f'First Principal Component ({pca.explained_variance_ratio[0]*100:.1f}%)')\n",
        "    # axes[0].set_ylabel(f'Second Principal Component ({pca.explained_variance_ratio[1]*100:.1f}%)')\n",
        "    # axes[0].set_title('Data Distribution by True Labels')\n",
        "    # axes[0].legend(*scatter1.legend_elements(), title=\"Heart Attack Risk\", loc='best')\n",
        "    # axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # # Plot 2: Colored by cluster labels\n",
        "    # scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels,\n",
        "    #                            cmap='viridis', alpha=0.6, edgecolors='k', linewidth=0.5)\n",
        "    # axes[1].set_xlabel(f'First Principal Component ({pca.explained_variance_ratio[0]*100:.1f}%)')\n",
        "    # axes[1].set_ylabel(f'Second Principal Component ({pca.explained_variance_ratio[1]*100:.1f}%)')\n",
        "    # axes[1].set_title(f'Data Distribution by Clusters (K={n_clusters})')\n",
        "    # axes[1].legend(*scatter2.legend_elements(), title=\"Cluster\", loc='best')\n",
        "    # axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # # Plot cluster centers in PCA space\n",
        "    # centers_pca = pca.transform(kmeans.centroids)\n",
        "    # axes[1].scatter(centers_pca[:, 0], centers_pca[:, 1],\n",
        "    #                c='red', marker='X', s=200, edgecolors='black', linewidth=2,\n",
        "    #                label='Cluster Centers')\n",
        "    # axes[1].legend()\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    # # Analyze cluster-label correspondence\n",
        "    # print(f\"\\n  Cluster-Label Correspondence:\")\n",
        "    # for i in range(n_clusters):\n",
        "    #     cluster_mask = cluster_labels == i\n",
        "    #     cluster_risk_rate = np.mean(y[cluster_mask])\n",
        "    #     print(f\"    Cluster {i}: {sum(cluster_mask)} samples, \"\n",
        "    #           f\"Risk rate: {cluster_risk_rate*100:.1f}%\")\n",
        "\n",
        "    # return pca, X_pca\n",
        "\n",
        "# TODO: After implementing PCA and K-means, test them on training data\n",
        "# Example usage:\n",
        "# visualize_clusters_with_pca(X_train, y_train.values, n_clusters=3)\n",
        "# Try different numbers of clusters: n_clusters=2, 4, 5"
      ],
      "metadata": {
        "id": "vJWUFqu9zp_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. Results Summary and Comparison (This is just an example. You may modify this if you want.)\n",
        "\n",
        "Compare all your models side-by-side.\n",
        "\n",
        "**What to compare:**\n",
        "- Accuracy, Precision, Recall, F1-Score\n",
        "- Memory usage"
      ],
      "metadata": {
        "id": "kAUCwXG_3PKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary and Results Comparison\n",
        "print(\"=\"*80)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\"\"\"\n",
        "Compare all your models and summarize the results\n",
        "\"\"\"\n",
        "\n",
        "def create_results_summary(results_dict):\n",
        "    \"\"\"\n",
        "    Create a summary table of all model results\n",
        "\n",
        "    Args:\n",
        "        results_dict: Dictionary with format {model_name: metrics_dict}\n",
        "    \"\"\"\n",
        "    if not results_dict:\n",
        "        print(\"  No results to summarize yet\")\n",
        "        return\n",
        "\n",
        "    summary_df = pd.DataFrame(results_dict).T\n",
        "    summary_df = summary_df.sort_values('accuracy', ascending=False)\n",
        "\n",
        "    print(\"\\nModel Performance Comparison:\")\n",
        "    print(\"=\"*80)\n",
        "    print(summary_df[['accuracy', 'precision', 'recall', 'f1_score', 'memory']])\n",
        "\n",
        "    # Visualize comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Plot 1: Performance metrics\n",
        "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']\n",
        "    summary_df[metrics_to_plot].plot(kind='bar', ax=axes[0], width=0.8)\n",
        "    axes[0].set_title('Model Performance Comparison')\n",
        "    axes[0].set_ylabel('Score')\n",
        "    axes[0].set_xlabel('Model')\n",
        "    axes[0].legend(loc='lower right')\n",
        "    axes[0].set_xticklabels(summary_df.index, rotation=45, ha='right')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].axhline(y=0.60, color='r', linestyle='--', label='Task 2 Threshold (60%)')\n",
        "\n",
        "    # Plot 2: Memory usage\n",
        "    summary_df['memory'].plot(kind='bar', ax=axes[1], color='orange', width=0.8)\n",
        "    axes[1].set_title('Model Memory Usage')\n",
        "    axes[1].set_ylabel('Memory')\n",
        "    axes[1].set_xlabel('Model')\n",
        "    axes[1].set_xticklabels(summary_df.index, rotation=45, ha='right')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "# TODO: After evaluating all your models, create a summary\n",
        "# Example:\n",
        "# results = {\n",
        "#     'Task 1 Model': metrics_task1,\n",
        "#     'Task 2 Model': metrics_task2,\n",
        "#     'Task 3 Model': metrics_task3,\n",
        "# }\n",
        "# summary = create_results_summary(results)"
      ],
      "metadata": {
        "id": "43Mje7NIz0hd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}